{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesseract\n",
    "Is a library from google (https://github.com/tesseract-ocr/tesseract) to perform OCR  \n",
    "We will leverage the Pytesseract bindings to tesseract via spark to perform distributed OCR against document strings in a dataframe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, \n",
    "We develop our algorithm locally  \n",
    "The file we are using is public domain shakespeare - about 100 pages long  \n",
    "For simplicities sake, we can just copy/paste this file a bunch of times to load the system. My system has only 8 logical cores, so I will only need about 8 files to complete the tutorial. Adjust as you see fit.    \n",
    "In reality, input files for a process like this can vary widely from a single page to over a thousand - which the last part of this tutorial deals with (skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import pdf2image\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "max_dop = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -O https://shakespeare.folger.edu/downloads/pdf/julius-caesar_PDF_FolgerShakespeare.pdf \n",
    "# TODO Adjust to max_dop  with shell magic\n",
    "! seq 8 | xargs -I XY cp ./by_the_sea_poe.pdf ./copy_by_the_sea_poeXY.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = list(map(lambda x: '/'.join([os.getcwd(),x]), filter(lambda x: x.endswith('.pdf'), list(os.walk(os.getcwd()))[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/bsavoy/pytesseract_on_pyspark/by_the_sea_poe.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe6.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe7.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe1.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe2.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe8.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe4.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe5.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_by_the_sea_poe3.pdf']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 3.9696993827819824s\n"
     ]
    }
   ],
   "source": [
    "content = pdf2image.convert_from_bytes(open(fnames[0],'rb').read())\n",
    "doc_string = ''\n",
    "start = time.time()\n",
    "for page in content:\n",
    "    doc_string += pytesseract.image_to_string(page)\n",
    "    \n",
    "elapsed = time.time() - start\n",
    "print('took {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how that scales on our box\n",
    "And I don't see that scaling well beyond a handful of documents at a time - perhaps for an ad-hoc process\n",
    "Perhaps performance will be better if we multiprocess it - pytesseract is a binding with the tesseract executable, afterall - sub-processes also sidestep interpreter lock.  \n",
    "We should be able to get some benefit by forking more external processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ocr(fname):\n",
    "    content = pdf2image.convert_from_bytes(open(fnames[0],'rb').read())\n",
    "    doc_string = ''\n",
    "    for page in content:\n",
    "        doc_string += pytesseract.image_to_string(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 9, 40.4643657207489],\n",
       " [2, 9, 23.20615577697754],\n",
       " [3, 9, 18.52650022506714],\n",
       " [4, 9, 21.658716678619385],\n",
       " [5, 9, 20.666146993637085],\n",
       " [6, 9, 23.207656621932983],\n",
       " [7, 9, 30.47342801094055],\n",
       " [8, 9, 287.7722792625427]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO convert to pandas and display over matplotlib\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take away\n",
    "The benefit of parallelism drops before we even saturate even the physical core count. Welcome to suggestions, but my first guess is context switching cost since the CPU is so busy between different tesseract threads.  \n",
    "Therefore, anecdotally, we have only up to 3 concurrent tesseract processes (which in reality, are using multiple cores as they run) before we start seeing a performance drop off which is less than the 4 physical cores of my CPU, let alone the 8 logical cores. Rougly 1/3 of the cores available on the box.      \n",
    "\n",
    "This is a small file - with only 2 pages. In other applications, you will could run into files of varying length. A 100 page sample here takes approximately 1000s  - which we can use to project that even for 50 documents on our box, with a parallelism of 3, we could take up to 50,000/3 or 16,667 seconds or 277 minutes. Even leveraging a larger box, with 96 cores, we can only use roughtly 1/3 of the available cores we would still end up with a 32nd degree of parallelism, roughly 1500s for a run or about 26 minutes. Hardly adequate for streaming or any other time sensitive work loads.  \n",
    "\n",
    "If we're dividng the work by document, the best, on average performance we could get for a run would be about 1000s (foreshadowing the last example).\n",
    "\n",
    "We also have a major disadvantage that this solution MUST run on single node.  \n",
    "We could split the workload across a number of hosts but this has orchestration and scheduling headaches    \n",
    "Also, what if you wanted to coherently deal with this data in a dataframe API after extracting the text?  \n",
    "Spark helps with these problems  \n",
    "* Spark can handle arbitrarily large datasets by natively splitting data across a cluster   \n",
    "* Spark can apply transformations in a distributed fashion and return a single coherent dataset back from our transformations  \n",
    "* The workload can scale to N number of nodes - we tune executors for the individual hosts we plan to have in the cluster and let spark loose to schedule the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dop=1 files=9 seconds=40.4643657207489\n",
      "dop=2 files=9 seconds=23.20615577697754\n",
      "dop=3 files=9 seconds=18.52650022506714\n",
      "dop=4 files=9 seconds=21.658716678619385\n",
      "dop=5 files=9 seconds=20.666146993637085\n",
      "dop=6 files=9 seconds=23.207656621932983\n",
      "dop=7 files=9 seconds=30.47342801094055\n",
      "dop=8 files=9 seconds=287.7722792625427\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "data = []\n",
    "# In my VM, I only have 8 cpu cores to work with (really 4 physical and 8 logical) available on my host\n",
    "for dop in range(1,9):\n",
    "    with Pool(dop) as p:\n",
    "        start = time.time()\n",
    "        p.map(do_ocr, fnames)\n",
    "        elapsed = time.time() - start\n",
    "        print('dop={} files={} seconds={}'.format(dop,len(fnames),elapsed))\n",
    "        data.append([dop,len(fnames),elapsed])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next,\n",
    "* We will package our OCR function into a method that Pyspark will pickle and distribute to worker nodes - eerily similarly to the do_ocr function above.    \n",
    "* In our driver program, we will instruct spark to read in the PDFs as binary data into an RDD  \n",
    "* We will use the built-in RDD map function to apply our ocr function to the binary content of the PDFs in each element  \n",
    "* When execution is complete - we will have a new RDD which consists of the parsed text by tesseract, along with some telemetry and error handling (for troublesome PDFs or PDFs that well, aren't PDFs)\n",
    "* Finally we can simply convert the RDD to a dataframe and use spark.sql api functions against our OCR'd text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_rdd = spark.sparkContext.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pytesseract(record):\n",
    "    import socket\n",
    "    import time\n",
    "    \n",
    "    file_name = record[0]\n",
    "    binary_content = record[1]\n",
    "    start_time = time.time()\n",
    "    try: \n",
    "        ocr_text = do_ocr(binary_content)\n",
    "    except Exception as e:\n",
    "        ocr_text  = str(e)\n",
    "    final_time = time.time() - start_time\n",
    "    \n",
    "    return [socket.gethostname(), file_name, ocr_text, str(final_time) + 's']\n",
    "\n",
    "def do_ocr(binary_content):\n",
    "    from pdf2image import convert_from_bytes\n",
    "    import pytesseract\n",
    "    \n",
    "    content = convert_from_bytes(binary_content)\n",
    "    doc_string = ''\n",
    "    for page in content:\n",
    "        doc_string += pytesseract.image_to_string(page)\n",
    "        \n",
    "    return doc_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last,\n",
    "In the programs current state, if a large document is encountered by a worker, there is a chance the rest of the cluster could finish their documents and the remaining document keep the stage open, bottlenecking work and wasting compute. Also, our best case performance (in the case of my box) would be 1000s for an entire run (average for a single document). How could we improve this?    \n",
    "\n",
    "With some minor complexity, we can digest documents to their component pages before processing and, instead of distributing the document for processing, distribute the pages for a document and recombine them by a document key at the end. This avoids strapping a node with a larger docuement and decreases our best case runtime to the length of time to process a single page (if we had enough nodes to process every single page from every single document at once time). For easy math, say a 1000s minimum for a run reduces to 1000page/s/100page = 10s/page - for a best case runtime of 10s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.binaryFiles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
