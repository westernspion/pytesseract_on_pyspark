{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesseract\n",
    "Is a library from google (https://github.com/tesseract-ocr/tesseract) to perform OCR  \n",
    "We will leverage the Pytesseract bindings to tesseract via spark to perform distributed OCR against document strings in a dataframe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, \n",
    "We develop our algorithm locally  \n",
    "The file we are using is public domain shakespeare - about 100 pages long  \n",
    "We can just copy/paste this file a bunch of times to load the system - but I can garuntee without a real cluster behind this spark context you will wish you hadn't  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import pdf2image\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  603k    0  603k    0     0   459k      0 --:--:--  0:00:01 --:--:--  459k\n"
     ]
    }
   ],
   "source": [
    "#! rm /home/bsavoy/Downloads/ocr_docs/copy_shakespeare* \n",
    "! curl -O https://shakespeare.folger.edu/downloads/pdf/julius-caesar_PDF_FolgerShakespeare.pdf \n",
    "! seq 8 | xargs -I XY cp ./julius-caesar_PDF_FolgerShakespeare.pdf ./copy_shakespeareXY.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = list(map(lambda x: '/'.join([os.getcwd(),x]), filter(lambda x: x.endswith('.pdf'), list(os.walk(os.getcwd()))[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare1.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare4.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/julius-caesar_PDF_FolgerShakespeare.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare7.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare3.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare2.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare8.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare5.pdf',\n",
       " '/home/bsavoy/pytesseract_on_pyspark/copy_shakespeare6.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pdf2image.convert_from_bytes(open(fnames[0],'rb').read())\n",
    "doc_string = ''\n",
    "start = time.time()\n",
    "for page in content:\n",
    "    doc_string += pytesseract.image_to_string(page)\n",
    "    \n",
    "elapsed = time.time() - start\n",
    "print('took {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmm, thats long for one document\n",
    "And I don't see that scaling well beyond a handful of documents at a time - perhaps for an ad-hoc process\n",
    "Perhaps performance will be better if we multiprocess it - pytesseract is a binding with the tesseract executable, afterall - sub-processes also sidestep interpreter lock.  \n",
    "We should be able to get some benefit by forking more external processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ocr(fname):\n",
    "    content = pdf2image.convert_from_bytes(open(fnames[0],'rb').read())\n",
    "    doc_string = ''\n",
    "    for page in content:\n",
    "        doc_string += pytesseract.image_to_string(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "data = []\n",
    "# In my VM, I only have 8 cpu cores to work with (really 4 physical and 8 logical) available on my host\n",
    "for dop in range(2,8):\n",
    "    with Pool(dop) as p:\n",
    "        start = time.time()\n",
    "        p.map(do_ocr, fnames)\n",
    "        elapsed = time.time() - start\n",
    "        print('dop={} files={} seconds={}'.format(dop,len(fnames),elapsed))\n",
    "        data.append(dop,len(fnames),elapsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Benefit of parallelism drops before we even saturate all logical cores - so optimally, we have only up to 4 concurrent tesseract processes\n",
    "before we start seeing a performance drop off\n",
    "\n",
    "We also have a major disadvantage that this solution MUST run on single node.  \n",
    "We could split the workload across a number of hosts but this has orchestration and scheduling headaches    \n",
    "Also, what if you wanted to coherently deal with this data in a dataframe API of some sort?  \n",
    "Spark helps with these problems  \n",
    "* Spark can handle arbitrarily large datasets by natively splitting data across a cluster   \n",
    "* Spark can apply transformations in a distributed fashion and return a single coherent dataset back from our transformations  \n",
    "* The workload can scale to N number of nodes - we tune executors for the individual hosts we plan to have in the cluster and let spark lose to schedule the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# for fname in fnames:\n",
    "#     print(list(fnames))\n",
    "#     print('Convert file={}'.format(fname))\n",
    "#     content = pdf2image.convert_from_bytes(open(fname,'rb').read())\n",
    "#     doc_string = ''\n",
    "#     for page in content:\n",
    "#         doc_string += pytesseract.image_to_string(page)\n",
    "# print('took {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next,\n",
    "* We will package our OCR function into a method that Pyspark will pickle and distribute to worker nodes - eerily similarly to the do_ocr function above.    \n",
    "* In our driver program, we will instruct spark to read in the PDFs as binary data into an RDD  \n",
    "* We will use the built-in RDD map function to apply our ocr function to the binary content of the PDFs in each element  \n",
    "* When execution is complete - we will have a new RDD which consists of the parsed text by tesseract, along with some telemetry and error handling (for troublesome PDFs or PDFs that well, aren't PDFs)\n",
    "* Finally we can simply convert the RDD to a dataframe and use spark.sql api functions against our OCR'd text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_rdd = spark.sparkContext.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pytesseract(record):\n",
    "    import socket\n",
    "    import time\n",
    "    \n",
    "    file_name = record[0]\n",
    "    binary_content = record[1]\n",
    "    start_time = time.time()\n",
    "    try: \n",
    "        ocr_text = do_ocr(binary_content)\n",
    "    except Exception as e:\n",
    "        ocr_text  = str(e)\n",
    "    final_time = time.time() - start_time\n",
    "    \n",
    "    return [socket.gethostname(), file_name, ocr_text, str(final_time) + 's']\n",
    "\n",
    "def do_ocr(binary_content):\n",
    "    from pdf2image import convert_from_bytes\n",
    "    import pytesseract\n",
    "    \n",
    "    content = convert_from_bytes(binary_content)\n",
    "    doc_string = ''\n",
    "    for page in content:\n",
    "        doc_string += pytesseract.image_to_string(page)\n",
    "        \n",
    "    return doc_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last,\n",
    "In the programs current state, if a large document is encountered by a worker, there is a chance the rest of the cluster could finish their documents and the remaining document keep the stage open, bottlenecking work and wasting compute  \n",
    "It's more efficient to distribute the pages for the document rather than the entire document, then coalesce the pages back together, in order, for a given document  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.binaryFiles()aaa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
